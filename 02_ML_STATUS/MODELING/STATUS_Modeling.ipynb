{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 1 - CONFIG AND LOAD DATA**"
      ],
      "metadata": {
        "id": "UZ7XKGfi0d3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# STATUS MODULE - MODELING (Model Comparison & Selection)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "#\n",
        "# OBIETTIVO:\n",
        "# Comparare 6 modelli per classificazione experience level, selezionare migliore\n",
        "#\n",
        "# INPUT:\n",
        "#   - models/status_preprocessed_v2.2.pkl (X_train/test, y_train/test, scaler)\n",
        "#\n",
        "# OUTPUT:\n",
        "#   - models/status_best_model_v2.2.pkl (best model trained)\n",
        "#   - models/status_model_comparison_v2.2.json (performance metrics)\n",
        "#   - visualizations/STATUS_Modeling_v2.2/ (confusion matrices, feature importance)\n",
        "#\n",
        "# MODELLI TESTATI:\n",
        "#   1. Dummy Classifier (baseline assoluto)\n",
        "#   2. Logistic Regression (linear baseline)\n",
        "#   3. Decision Tree (interpretable, single tree)\n",
        "#   4. Random Forest (bagging ensemble)\n",
        "#   5. Gradient Boosting (sklearn boosting baseline)\n",
        "#   6. XGBoost (candidate finale, regularized)\n",
        "#\n",
        "# METRICHE:\n",
        "#   - Accuracy (test set)\n",
        "#   - F1-macro, F1 per-class\n",
        "#   - Train-test gap (overfitting check)\n",
        "#   - Training time\n",
        "#   - Feature importance (tree-based models)\n",
        "#\n",
        "# VERSIONE: 2.2\n",
        "# DATA: 2026-02-09\n",
        "# AUTORE: Alessandro Ambrosio\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STATUS MODULE - MODELING (Model Comparison)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PIuU7TU0ihd",
        "outputId": "96fb4853-d408-45b5-8fab-d30d256fe91d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATUS MODULE - MODELING (Model Comparison)\n",
            "================================================================================\n",
            "Timestamp: 2026-02-16 00:54:43\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 1: LOAD PREPROCESSED DATA\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 1: LOAD PREPROCESSED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Paths\n",
        "MODELDIR = Path('models')\n",
        "VIZDIR = Path('visualizations/STATUS_Modeling_v2.2')\n",
        "VIZDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load preprocessed data\n",
        "preprocessed_path = MODELDIR / 'status_preprocessed_v2.2.pkl'\n",
        "\n",
        "print(f\"\\n[OK] Loading: {preprocessed_path}\")\n",
        "\n",
        "with open(preprocessed_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "scaler = data['scaler']\n",
        "feature_names = data['feature_names']\n",
        "target_encoding = data['target_encoding']\n",
        "\n",
        "print(f\"\\n[OK] X_train shape: {X_train.shape}\")\n",
        "print(f\"[OK] X_test shape: {X_test.shape}\")\n",
        "print(f\"[OK] Features: {len(feature_names)}\")\n",
        "print(f\"[OK] Features: {feature_names}\")\n",
        "\n",
        "# Target distribution\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TARGET DISTRIBUTION\")\n",
        "print(\"-\"*80)\n",
        "print(\"\\nTrain:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(\"\\nTest:\")\n",
        "print(y_test.value_counts().sort_index())\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Teuh5nrD0-HK",
        "outputId": "bdab8e3c-3fad-4c95-9c78-7085b94c5219"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 1: LOAD PREPROCESSED DATA\n",
            "================================================================================\n",
            "\n",
            "[OK] Loading: models/status_preprocessed_v2.2.pkl\n",
            "\n",
            "[OK] X_train shape: (408, 7)\n",
            "[OK] X_test shape: (102, 7)\n",
            "[OK] Features: 7\n",
            "[OK] Features: ['reps_mean', 'rpe_mean', 'total_sets', 'acwr_mean', 'spike_weeks_count', 'load_progression', 'skip_rate']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TARGET DISTRIBUTION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Train:\n",
            "experience_label\n",
            "Advanced        136\n",
            "Beginner        136\n",
            "Intermediate    136\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test:\n",
            "experience_label\n",
            "Advanced        34\n",
            "Beginner        34\n",
            "Intermediate    34\n",
            "Name: count, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 3 - BASELINE MODELS**"
      ],
      "metadata": {
        "id": "bxvQQwS30hjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 2: BASELINE MODELS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 2: BASELINE MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Storage for results\n",
        "results = []\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 1: Dummy Classifier (Stratified)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[1/6] Dummy Classifier (Stratified Baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dummy.predict(X_train)\n",
        "y_test_pred = dummy.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Dummy (Stratified)',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 2: Logistic Regression\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[2/6] Logistic Regression (Linear Baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    solver='lbfgs',\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = logreg.predict(X_train)\n",
        "y_test_pred = logreg.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Logistic Regression',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 3: Decision Tree\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[3/6] Decision Tree (Interpretable Baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Decision Tree',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RTOBK9s0jcD",
        "outputId": "5449d1d8-dd73-4351-cc2c-c5c63d4fa30e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 2: BASELINE MODELS\n",
            "================================================================================\n",
            "\n",
            "[1/6] Dummy Classifier (Stratified Baseline)...\n",
            "  Train Accuracy: 0.368\n",
            "  Test Accuracy:  0.294\n",
            "  Test F1-macro:  0.295\n",
            "  Time: 0.02s\n",
            "\n",
            "[2/6] Logistic Regression (Linear Baseline)...\n",
            "  Train Accuracy: 0.968\n",
            "  Test Accuracy:  0.931\n",
            "  Test F1-macro:  0.931\n",
            "  Gap: 0.037\n",
            "  Time: 0.12s\n",
            "\n",
            "[3/6] Decision Tree (Interpretable Baseline)...\n",
            "  Train Accuracy: 0.944\n",
            "  Test Accuracy:  0.912\n",
            "  Test F1-macro:  0.912\n",
            "  Gap: 0.032\n",
            "  Time: 0.06s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 4 - ENSEMBLE METHODS**"
      ],
      "metadata": {
        "id": "_GhGcnEO0hlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 3: ENSEMBLE METHODS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 3: ENSEMBLE METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 4: Random Forest\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[4/6] Random Forest (Bagging Ensemble)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Random Forest',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 5: Gradient Boosting (sklearn)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[5/6] Gradient Boosting (sklearn baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = gb.predict(X_train)\n",
        "y_test_pred = gb.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Gradient Boosting',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 6: XGBoost (Candidate Finale)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[6/6] XGBoost (Regularized, Candidate Finale)...\")\n",
        "\n",
        "# XGBoost requires numeric labels\n",
        "label_encoder = {'Beginner': 0, 'Intermediate': 1, 'Advanced': 2}\n",
        "y_train_encoded = y_train.map(label_encoder)\n",
        "y_test_encoded = y_test.map(label_encoder)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=150,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,          # L1 regularization\n",
        "    reg_lambda=1.0,         # L2 regularization\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "xgb.fit(X_train, y_train_encoded)\n",
        "\n",
        "y_train_pred_encoded = xgb.predict(X_train)\n",
        "y_test_pred_encoded = xgb.predict(X_test)\n",
        "\n",
        "# Decode back to string labels for consistency\n",
        "reverse_encoder = {0: 'Beginner', 1: 'Intermediate', 2: 'Advanced'}\n",
        "y_train_pred = pd.Series(y_train_pred_encoded).map(reverse_encoder)\n",
        "y_test_pred = pd.Series(y_test_pred_encoded).map(reverse_encoder)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'XGBoost',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTDTUVzJ0j3l",
        "outputId": "cd696c11-8030-494e-f46e-1f18f3dc2d70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 3: ENSEMBLE METHODS\n",
            "================================================================================\n",
            "\n",
            "[4/6] Random Forest (Bagging Ensemble)...\n",
            "  Train Accuracy: 0.985\n",
            "  Test Accuracy:  0.941\n",
            "  Test F1-macro:  0.941\n",
            "  Gap: 0.044\n",
            "  Time: 0.87s\n",
            "\n",
            "[5/6] Gradient Boosting (sklearn baseline)...\n",
            "  Train Accuracy: 1.000\n",
            "  Test Accuracy:  0.951\n",
            "  Test F1-macro:  0.951\n",
            "  Gap: 0.049\n",
            "  Time: 1.49s\n",
            "\n",
            "[6/6] XGBoost (Regularized, Candidate Finale)...\n",
            "  Train Accuracy: 1.000\n",
            "  Test Accuracy:  0.931\n",
            "  Test F1-macro:  0.931\n",
            "  Gap: 0.069\n",
            "  Time: 0.25s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 5 - MODEL COMPARISON (Extended Metrics)**"
      ],
      "metadata": {
        "id": "SrOkx_sl0hqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 4: MODEL COMPARISON (Extended Metrics)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 4: MODEL COMPARISON (Extended Metrics)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Re-train all models and collect extended metrics\n",
        "models_dict = {\n",
        "    'Dummy (Stratified)': DummyClassifier(strategy='stratified', random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs', multi_class='multinomial'),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_split=10, min_samples_leaf=4, max_features='sqrt', random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=4, min_samples_split=10, min_samples_leaf=4, subsample=0.8, random_state=42),\n",
        "    'XGBoost': XGBClassifier(n_estimators=150, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
        "}\n",
        "\n",
        "# Extended results storage\n",
        "extended_results = []\n",
        "\n",
        "print(\"\\nTraining models with extended metrics...\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"\\n{model_name}...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Handle XGBoost encoding\n",
        "    if model_name == 'XGBoost':\n",
        "        label_encoder = {'Beginner': 0, 'Intermediate': 1, 'Advanced': 2}\n",
        "        y_train_encoded = y_train.map(label_encoder)\n",
        "        y_test_encoded = y_test.map(label_encoder)\n",
        "\n",
        "        model.fit(X_train, y_train_encoded)\n",
        "\n",
        "        y_train_pred_encoded = model.predict(X_train)\n",
        "        y_test_pred_encoded = model.predict(X_test)\n",
        "\n",
        "        reverse_encoder = {0: 'Beginner', 1: 'Intermediate', 2: 'Advanced'}\n",
        "        y_train_pred = pd.Series(y_train_pred_encoded).map(reverse_encoder).values\n",
        "        y_test_pred = pd.Series(y_test_pred_encoded).map(reverse_encoder).values\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
        "    test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
        "    test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    # Per-class metrics (test set only)\n",
        "    test_precision_per_class = precision_score(y_test, y_test_pred, average=None, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "    test_recall_per_class = recall_score(y_test, y_test_pred, average=None, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "    test_f1_per_class = f1_score(y_test, y_test_pred, average=None, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "\n",
        "    print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "    print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "    print(f\"  Test Precision: {test_precision:.3f}\")\n",
        "    print(f\"  Test Recall:    {test_recall:.3f}\")\n",
        "    print(f\"  Train-Test Gap: {train_acc - test_acc:.3f}\")\n",
        "    print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "    extended_results.append({\n",
        "        'model': model_name,\n",
        "        'train_acc': train_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'train_f1': train_f1,\n",
        "        'test_f1': test_f1,\n",
        "        'train_precision': train_precision,\n",
        "        'test_precision': test_precision,\n",
        "        'train_recall': train_recall,\n",
        "        'test_recall': test_recall,\n",
        "        'gap': train_acc - test_acc,\n",
        "        'time_sec': elapsed,\n",
        "        'test_precision_beginner': test_precision_per_class[0],\n",
        "        'test_precision_intermediate': test_precision_per_class[1],\n",
        "        'test_precision_advanced': test_precision_per_class[2],\n",
        "        'test_recall_beginner': test_recall_per_class[0],\n",
        "        'test_recall_intermediate': test_recall_per_class[1],\n",
        "        'test_recall_advanced': test_recall_per_class[2],\n",
        "        'test_f1_beginner': test_f1_per_class[0],\n",
        "        'test_f1_intermediate': test_f1_per_class[1],\n",
        "        'test_f1_advanced': test_f1_per_class[2]\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_results = pd.DataFrame(extended_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON TABLE (Test Set)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Display main metrics\n",
        "print(\"\\nOverall Performance:\")\n",
        "print(df_results[['model', 'test_acc', 'test_f1', 'test_precision', 'test_recall', 'gap', 'time_sec']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Per-Class Performance (Test Set)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Display per-class metrics\n",
        "print(\"\\nPrecision per class:\")\n",
        "print(df_results[['model', 'test_precision_beginner', 'test_precision_intermediate', 'test_precision_advanced']].to_string(index=False))\n",
        "\n",
        "print(\"\\nRecall per class:\")\n",
        "print(df_results[['model', 'test_recall_beginner', 'test_recall_intermediate', 'test_recall_advanced']].to_string(index=False))\n",
        "\n",
        "print(\"\\nF1-Score per class:\")\n",
        "print(df_results[['model', 'test_f1_beginner', 'test_f1_intermediate', 'test_f1_advanced']].to_string(index=False))\n",
        "\n",
        "# Identify best model (by test F1-macro, considering gap < 0.10)\n",
        "df_filtered = df_results[df_results['gap'] < 0.10]  # Filter overfitting models\n",
        "if len(df_filtered) > 0:\n",
        "    best_model_name = df_filtered.loc[df_filtered['test_f1'].idxmax(), 'model']\n",
        "else:\n",
        "    best_model_name = df_results.loc[df_results['test_f1'].idxmax(), 'model']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Selection criteria: Highest F1-macro with train-test gap < 0.10\")\n",
        "\n",
        "# Save comparison results\n",
        "comparison_path = MODELDIR / 'status_model_comparison_v2.2.json'\n",
        "df_results.to_json(comparison_path, orient='records', indent=2)\n",
        "print(f\"\\n[OK] Comparison saved: {comparison_path}\")\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YnIoce0kLg",
        "outputId": "4ba3d2c3-4f40-4231-8eaf-15372cb5125c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 4: MODEL COMPARISON (Extended Metrics)\n",
            "================================================================================\n",
            "\n",
            "Training models with extended metrics...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Dummy (Stratified)...\n",
            "  Test Accuracy:  0.294\n",
            "  Test F1-macro:  0.295\n",
            "  Test Precision: 0.297\n",
            "  Test Recall:    0.294\n",
            "  Train-Test Gap: 0.074\n",
            "  Time: 0.00s\n",
            "\n",
            "Logistic Regression...\n",
            "  Test Accuracy:  0.931\n",
            "  Test F1-macro:  0.931\n",
            "  Test Precision: 0.932\n",
            "  Test Recall:    0.931\n",
            "  Train-Test Gap: 0.037\n",
            "  Time: 0.01s\n",
            "\n",
            "Decision Tree...\n",
            "  Test Accuracy:  0.912\n",
            "  Test F1-macro:  0.912\n",
            "  Test Precision: 0.914\n",
            "  Test Recall:    0.912\n",
            "  Train-Test Gap: 0.032\n",
            "  Time: 0.01s\n",
            "\n",
            "Random Forest...\n",
            "  Test Accuracy:  0.941\n",
            "  Test F1-macro:  0.941\n",
            "  Test Precision: 0.942\n",
            "  Test Recall:    0.941\n",
            "  Train-Test Gap: 0.044\n",
            "  Time: 0.76s\n",
            "\n",
            "Gradient Boosting...\n",
            "  Test Accuracy:  0.951\n",
            "  Test F1-macro:  0.951\n",
            "  Test Precision: 0.951\n",
            "  Test Recall:    0.951\n",
            "  Train-Test Gap: 0.049\n",
            "  Time: 1.33s\n",
            "\n",
            "XGBoost...\n",
            "  Test Accuracy:  0.931\n",
            "  Test F1-macro:  0.931\n",
            "  Test Precision: 0.933\n",
            "  Test Recall:    0.931\n",
            "  Train-Test Gap: 0.069\n",
            "  Time: 0.16s\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON TABLE (Test Set)\n",
            "================================================================================\n",
            "\n",
            "Overall Performance:\n",
            "              model  test_acc  test_f1  test_precision  test_recall      gap  time_sec\n",
            " Dummy (Stratified)  0.294118 0.294965        0.296908     0.294118 0.073529  0.002465\n",
            "Logistic Regression  0.931373 0.931050        0.931548     0.931373 0.036765  0.011492\n",
            "      Decision Tree  0.911765 0.911833        0.913850     0.911765 0.031863  0.009177\n",
            "      Random Forest  0.941176 0.940862        0.941653     0.941176 0.044118  0.755886\n",
            "  Gradient Boosting  0.950980 0.950812        0.951488     0.950980 0.049020  1.331050\n",
            "            XGBoost  0.931373 0.931019        0.932634     0.931373 0.068627  0.160325\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Per-Class Performance (Test Set)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Precision per class:\n",
            "              model  test_precision_beginner  test_precision_intermediate  test_precision_advanced\n",
            " Dummy (Stratified)                 0.354839                     0.263158                 0.272727\n",
            "Logistic Regression                 0.971429                     0.885714                 0.937500\n",
            "      Decision Tree                 0.916667                     0.857143                 0.967742\n",
            "      Random Forest                 0.944444                     0.911765                 0.968750\n",
            "  Gradient Boosting                 0.971429                     0.914286                 0.968750\n",
            "            XGBoost                 0.944444                     0.885714                 0.967742\n",
            "\n",
            "Recall per class:\n",
            "              model  test_recall_beginner  test_recall_intermediate  test_recall_advanced\n",
            " Dummy (Stratified)              0.323529                  0.294118              0.264706\n",
            "Logistic Regression              1.000000                  0.911765              0.882353\n",
            "      Decision Tree              0.970588                  0.882353              0.882353\n",
            "      Random Forest              1.000000                  0.911765              0.911765\n",
            "  Gradient Boosting              1.000000                  0.941176              0.911765\n",
            "            XGBoost              1.000000                  0.911765              0.882353\n",
            "\n",
            "F1-Score per class:\n",
            "              model  test_f1_beginner  test_f1_intermediate  test_f1_advanced\n",
            " Dummy (Stratified)          0.338462              0.277778          0.268657\n",
            "Logistic Regression          0.985507              0.898551          0.909091\n",
            "      Decision Tree          0.942857              0.869565          0.923077\n",
            "      Random Forest          0.971429              0.911765          0.939394\n",
            "  Gradient Boosting          0.985507              0.927536          0.939394\n",
            "            XGBoost          0.971429              0.898551          0.923077\n",
            "\n",
            "================================================================================\n",
            "BEST MODEL: Gradient Boosting\n",
            "================================================================================\n",
            "Selection criteria: Highest F1-macro with train-test gap < 0.10\n",
            "\n",
            "[OK] Comparison saved: models/status_model_comparison_v2.2.json\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 6 - VISUALIZATIONS**"
      ],
      "metadata": {
        "id": "sJgyDRtJ0hsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDjcRZqu0ZG5",
        "outputId": "d53a1791-2b56-4f6f-8b6e-479ca43e7ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 5: VISUALIZATIONS\n",
            "================================================================================\n",
            "\n",
            "Re-training best model (Gradient Boosting) for analysis...\n",
            "\n",
            "[1/3] Confusion Matrix (STATUS Branding)...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "CONFUSION MATRIX - NUMERICAL ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Raw Confusion Matrix:\n",
            "              Beginner  Intermediate  Advanced\n",
            "Beginner            34             0         0\n",
            "Intermediate         1            32         1\n",
            "Advanced             0             3        31\n",
            "\n",
            "Overall Accuracy: 0.9510\n",
            "\n",
            "Per-Class Metrics:\n",
            "\n",
            "Beginner\n",
            "  Precision: 0.9714\n",
            "  Recall   : 1.0000\n",
            "  F1-Score : 0.9855\n",
            "\n",
            "Intermediate\n",
            "  Precision: 0.9143\n",
            "  Recall   : 0.9412\n",
            "  F1-Score : 0.9275\n",
            "\n",
            "Advanced\n",
            "  Precision: 0.9688\n",
            "  Recall   : 0.9118\n",
            "  F1-Score : 0.9394\n",
            "  [OK] Saved: visualizations/STATUS_Modeling_v2.2/confusion_matrix_best_model_v2.2.png\n",
            "\n",
            "[2/3] Feature Importance...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "FEATURE IMPORTANCE - NUMERICAL RANKING\n",
            "--------------------------------------------------------------------------------\n",
            " 1. skip_rate                 | Importance: 0.6146 | Contribution: 61.46%\n",
            " 2. reps_mean                 | Importance: 0.1354 | Contribution: 13.54%\n",
            " 3. total_sets                | Importance: 0.1289 | Contribution: 12.89%\n",
            " 4. acwr_mean                 | Importance: 0.0553 | Contribution: 5.53%\n",
            " 5. rpe_mean                  | Importance: 0.0318 | Contribution: 3.18%\n",
            " 6. load_progression          | Importance: 0.0285 | Contribution: 2.85%\n",
            " 7. spike_weeks_count         | Importance: 0.0055 | Contribution: 0.55%\n",
            "\n",
            "Cumulative Importance (Top Features):\n",
            "Top  1 → 0.6146\n",
            "Top  2 → 0.7500\n",
            "Top  3 → 0.8789\n",
            "Top  4 → 0.9342\n",
            "Top  5 → 0.9660\n",
            "Top  6 → 0.9945\n",
            "Top  7 → 1.0000\n",
            "\n",
            "Top 3 features contribution: 87.89%\n",
            "[WARNING] Model heavily driven by few features\n",
            "  [OK] Saved: visualizations/STATUS_Modeling_v2.2/feature_importance_best_model_v2.2.png\n",
            "\n",
            "Top 3 Features:\n",
            "  1. skip_rate           : 0.615\n",
            "  2. reps_mean           : 0.135\n",
            "  3. total_sets          : 0.129\n",
            "\n",
            "[3/3] Model Comparison Chart...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL COMPARISON - NUMERICAL ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Model: Gradient Boosting\n",
            "  Train Accuracy : 1.0000\n",
            "  Test Accuracy  : 0.9510\n",
            "  Generalization Gap: 0.0490\n",
            "  [GOOD GENERALIZATION]\n",
            "\n",
            "Model: Random Forest\n",
            "  Train Accuracy : 0.9853\n",
            "  Test Accuracy  : 0.9412\n",
            "  Generalization Gap: 0.0441\n",
            "  [GOOD GENERALIZATION]\n",
            "\n",
            "Model: Logistic Regression\n",
            "  Train Accuracy : 0.9681\n",
            "  Test Accuracy  : 0.9314\n",
            "  Generalization Gap: 0.0368\n",
            "  [GOOD GENERALIZATION]\n",
            "\n",
            "Model: XGBoost\n",
            "  Train Accuracy : 1.0000\n",
            "  Test Accuracy  : 0.9314\n",
            "  Generalization Gap: 0.0686\n",
            "  [MILD GAP]\n",
            "\n",
            "Model: Decision Tree\n",
            "  Train Accuracy : 0.9436\n",
            "  Test Accuracy  : 0.9118\n",
            "  Generalization Gap: 0.0319\n",
            "  [GOOD GENERALIZATION]\n",
            "\n",
            "----------------------------------------\n",
            "BEST MODEL: Gradient Boosting\n",
            "Test Accuracy: 0.9510\n",
            "----------------------------------------\n",
            "  [OK] Saved:  visualizations/STATUS_Modeling_v2.2/model_comparison_v2.2.png\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 5: VISUALIZATIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 5: VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# STATUS Brand Colors\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "STATUS_COLORS = {\n",
        "    'navy': '#2B4162',\n",
        "    'royal_blue': '#385F8F',\n",
        "    'purple': '#7B5E9D',\n",
        "    'light_purple': '#9B7EBD',\n",
        "    'text_dark': '#1A1A1A',\n",
        "    'text_light': '#FFFFFF'\n",
        "}\n",
        "\n",
        "STATUS_PALETTE = ['#2B4162', '#7B5E9D', '#9B7EBD']  # Beginner, Intermediate, Advanced\n",
        "\n",
        "# Custom colormap (blue-purple gradient)\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "status_cmap = LinearSegmentedColormap.from_list(\n",
        "    'status',\n",
        "    ['#E8EAF6', '#9B7EBD', '#7B5E9D', '#385F8F', '#2B4162']\n",
        ")\n",
        "\n",
        "# Set seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(STATUS_PALETTE)\n",
        "\n",
        "# Re-train best model (Gradient Boosting) for visualization\n",
        "print(\"\\nRe-training best model (Gradient Boosting) for analysis...\")\n",
        "\n",
        "gb_best = GradientBoostingClassifier(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "gb_best.fit(X_train, y_train)\n",
        "y_test_pred = gb_best.predict(X_test)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# 5.1 Confusion Matrix (STATUS Branding, No Grid)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[1/3] Confusion Matrix (STATUS Branding)...\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# NUMERICAL CONFUSION MATRIX ANALYSIS\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"CONFUSION MATRIX - NUMERICAL ANALYSIS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred,\n",
        "                      labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "\n",
        "classes = ['Beginner', 'Intermediate', 'Advanced']\n",
        "\n",
        "print(\"\\nRaw Confusion Matrix:\")\n",
        "print(pd.DataFrame(cm, index=classes, columns=classes))\n",
        "\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "print(\"\\nPer-Class Metrics:\")\n",
        "for i, cls in enumerate(classes):\n",
        "    tp = cm[i, i]\n",
        "    fn = cm[i, :].sum() - tp\n",
        "    fp = cm[:, i].sum() - tp\n",
        "    tn = cm.sum() - (tp + fn + fp)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\n{cls}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall   : {recall:.4f}\")\n",
        "    print(f\"  F1-Score : {f1:.4f}\")\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "\n",
        "# Custom colormap for confusion matrix\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=['Beginner', 'Intermediate', 'Advanced']\n",
        ")\n",
        "disp.plot(cmap=status_cmap, ax=ax, values_format='d', colorbar=False)\n",
        "\n",
        "ax.grid(False)\n",
        "\n",
        "# Customize title and labels\n",
        "ax.set_title('STATUS Module - Confusion Matrix\\nGradient Boosting (Test Set)',\n",
        "             fontsize=16, weight='bold', color=STATUS_COLORS['navy'], pad=20)\n",
        "ax.set_xlabel('Predicted Experience Level', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "ax.set_ylabel('True Experience Level', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "\n",
        "# Add accuracy annotation\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "ax.text(0.5, -0.15, f'Accuracy: {accuracy:.1%}',\n",
        "        transform=ax.transAxes, ha='center', fontsize=12,\n",
        "        weight='bold', color=STATUS_COLORS['purple'])\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "cm_path = VIZDIR / 'confusion_matrix_best_model_v2.2.png'\n",
        "plt.savefig(cm_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"  [OK] Saved: {cm_path}\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# 5.2 Feature Importance\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[2/3] Feature Importance...\")\n",
        "\n",
        "feature_importance = gb_best.feature_importances_\n",
        "feature_df = (pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "  .reset_index(drop=True))\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# NUMERICAL FEATURE IMPORTANCE ANALYSIS\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"FEATURE IMPORTANCE - NUMERICAL RANKING\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "feature_df = feature_df.reset_index(drop=True)\n",
        "\n",
        "total_importance = feature_df['importance'].sum()\n",
        "\n",
        "for i, row in feature_df.iterrows():\n",
        "    perc = (row['importance'] / total_importance) * 100\n",
        "    print(f\"{i+1:2d}. {row['feature']:25s} \"\n",
        "          f\"| Importance: {row['importance']:.4f} \"\n",
        "          f\"| Contribution: {perc:.2f}%\")\n",
        "\n",
        "print(\"\\nCumulative Importance (Top Features):\")\n",
        "feature_df['cumulative'] = feature_df['importance'].cumsum()\n",
        "\n",
        "for i in range(len(feature_df)):\n",
        "    print(f\"Top {i+1:2d} → {feature_df.loc[i, 'cumulative']:.4f}\")\n",
        "\n",
        "# Dominance check\n",
        "top3 = feature_df.head(3)['importance'].sum()\n",
        "print(f\"\\nTop 3 features contribution: {(top3/total_importance)*100:.2f}%\")\n",
        "\n",
        "if top3/total_importance > 0.70:\n",
        "    print(\"[WARNING] Model heavily driven by few features\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 7))\n",
        "\n",
        "# Gradient colors (most important = darkest)\n",
        "n_features = len(feature_df)\n",
        "colors = [STATUS_COLORS['navy'] if i == 0\n",
        "          else STATUS_COLORS['royal_blue'] if i == 1\n",
        "          else STATUS_COLORS['purple'] if i == 2\n",
        "          else STATUS_COLORS['light_purple']\n",
        "          for i in range(n_features)]\n",
        "\n",
        "bars = ax.barh(feature_df['feature'], feature_df['importance'], color=colors, edgecolor='white', linewidth=1.5)\n",
        "\n",
        "ax.set_xlabel('Importance Score', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "ax.set_title('STATUS Module - Feature Importance\\nGradient Boosting Classifier',\n",
        "             fontsize=16, weight='bold', color=STATUS_COLORS['navy'], pad=20)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlim([0, max(feature_df['importance']) * 1.15])\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, feature_df['importance'])):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "            f'{val:.3f}',\n",
        "            ha='left', va='center', fontsize=11, weight='bold',\n",
        "            color=STATUS_COLORS['navy'])\n",
        "\n",
        "# Add ranking numbers\n",
        "for i, bar in enumerate(bars):\n",
        "    ax.text(0.685, bar.get_y() + bar.get_height()/2,\n",
        "            f'#{i+1}',\n",
        "            ha='right', va='center', fontsize=10, weight='bold',\n",
        "            color=STATUS_COLORS['purple'])\n",
        "\n",
        "# Grid styling\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--', color=STATUS_COLORS['royal_blue'])\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fi_path = VIZDIR / 'feature_importance_best_model_v2.2.png'\n",
        "plt.savefig(fi_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"  [OK] Saved: {fi_path}\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nTop 3 Features:\")\n",
        "for i, row in feature_df.head(3).iterrows():\n",
        "    print(f\"  {i+1}. {row['feature']:20s}: {row['importance']:.3f}\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# 5.3 Model Comparison Bar Chart\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[3/3] Model Comparison Chart...\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# NUMERICAL MODEL COMPARISON\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"MODEL COMPARISON - NUMERICAL ANALYSIS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "models_to_plot = df_results[df_results['model'] != 'Dummy (Stratified)'].copy()\n",
        "models_to_plot = models_to_plot.sort_values('test_acc', ascending=False)\n",
        "\n",
        "for _, row in models_to_plot.iterrows():\n",
        "\n",
        "    gap = row['train_acc'] - row['test_acc']\n",
        "\n",
        "    print(f\"\\nModel: {row['model']}\")\n",
        "    print(f\"  Train Accuracy : {row['train_acc']:.4f}\")\n",
        "    print(f\"  Test Accuracy  : {row['test_acc']:.4f}\")\n",
        "    print(f\"  Generalization Gap: {gap:.4f}\")\n",
        "\n",
        "    if gap >= 0.10:\n",
        "        print(\"  [OVERFITTING RISK]\")\n",
        "    elif gap >= 0.05:\n",
        "        print(\"  [MILD GAP]\")\n",
        "    else:\n",
        "        print(\"  [GOOD GENERALIZATION]\")\n",
        "\n",
        "\n",
        "best_model = models_to_plot.iloc[0]\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(f\"BEST MODEL: {best_model['model']}\")\n",
        "print(f\"Test Accuracy: {best_model['test_acc']:.4f}\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "models_to_plot = df_results[df_results['model'] != 'Dummy (Stratified)'].copy()\n",
        "models_to_plot = models_to_plot.sort_values('test_f1', ascending=False)\n",
        "best_model = models_to_plot.iloc[0]\n",
        "\n",
        "y_pos = np.arange(len(models_to_plot))\n",
        "\n",
        "# Train accuracy (background, lighter)\n",
        "bars_train = ax.barh(y_pos, models_to_plot['train_acc'],\n",
        "                      color=STATUS_COLORS['light_purple'], alpha=0.4,\n",
        "                      label='Train Accuracy', edgecolor='white', linewidth=1.5)\n",
        "\n",
        "# Test accuracy (foreground, darker)\n",
        "bars_test = ax.barh(y_pos, models_to_plot['test_acc'],\n",
        "                     color=STATUS_COLORS['navy'], alpha=0.85,\n",
        "                     label='Test Accuracy', edgecolor='white', linewidth=1.5)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(models_to_plot['model'], fontsize=11, weight='bold')\n",
        "ax.set_xlabel('Accuracy', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "ax.set_title('STATUS Module - Model Comparison\\nTrain vs Test Accuracy',\n",
        "             fontsize=16, weight='bold', color=STATUS_COLORS['navy'], pad=20)\n",
        "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
        "ax.set_xlim([0.85, 1.02])\n",
        "\n",
        "# Add value labels (test accuracy only)\n",
        "for i, test in enumerate(models_to_plot['test_acc']):\n",
        "    ax.text(test + 0.003, i, f'{test:.1%}',\n",
        "            va='center', fontsize=11, weight='bold',\n",
        "            color=STATUS_COLORS['navy'])\n",
        "\n",
        "# Highlight best model\n",
        "best_idx = models_to_plot['test_acc'].idxmax()\n",
        "best_pos = np.where(models_to_plot.index == best_idx)[0][0]\n",
        "ax.axhline(best_pos, color=STATUS_COLORS['purple'], linewidth=3, alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add \"BEST\" annotation\n",
        "ax.text(0.99, best_pos, '  ★ BEST', va='center', ha='right',\n",
        "        fontsize=10, weight='bold', color=STATUS_COLORS['purple'],\n",
        "        bbox=dict(boxstyle='round,pad=0.3', facecolor=STATUS_COLORS['light_purple'],\n",
        "                  alpha=0.3, edgecolor=STATUS_COLORS['purple']))\n",
        "\n",
        "# Grid styling\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--', color=STATUS_COLORS['royal_blue'])\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "comp_path = VIZDIR / 'model_comparison_v2.2.png'\n",
        "plt.savefig(comp_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"  [OK] Saved:  {comp_path}\")\n",
        "plt.close()\n",
        "\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 7 - MODEL SELECTION RATIONALE & SAVE**"
      ],
      "metadata": {
        "id": "kVAdtrdN8Qyl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccdlMR1X8Qyl",
        "outputId": "d3ab4f8e-18a7-44de-fcf3-41d07309795f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 6: MODEL SELECTION RATIONALE & SAVE\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "BEST MODEL: Gradient Boosting\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Why Gradient Boosting over alternatives:\n",
            "[OK] Test accuracy: 95.1%\n",
            "[OK] Test F1-macro: 0.951\n",
            "[OK] Beginner recall: 100.0%\n",
            "[OK] Advanced precision: 96.9%\n",
            "[OK] Train-test gap: 0.049 (< 0.10 threshold)\n",
            "[OK] Handles non-linear interactions (skip_rate × total_sets)\n",
            "\n",
            "Why NOT Logistic Regression:\n",
            "[X] Lower test accuracy: 93.1% vs 95.1%\n",
            "[OK] Better gap: 0.037 vs 0.049\n",
            "\n",
            "Why NOT XGBoost:\n",
            "[X] Larger gap: 0.069 (threshold 0.10)\n",
            "[X] Lower test accuracy: 93.1% vs 95.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAVING BEST MODEL\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[OK] Best model saved: models/status_best_model_v2.2.pkl\n",
            "  File size: 729.5 KB\n",
            "[OK] Model info saved: models/status_model_info_v2.2.json\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 6: MODEL SELECTION RATIONALE & SAVE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 6: MODEL SELECTION RATIONALE & SAVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        ")\n",
        "\n",
        "# --- Metrics + report ---\n",
        "y_train_pred = gb_best.predict(X_train)\n",
        "y_test_pred  = gb_best.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
        "test_precision_macro = precision_score(y_test, y_test_pred, average='macro')\n",
        "test_recall_macro = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_test_gap = train_accuracy - test_accuracy\n",
        "\n",
        "report = classification_report(y_test, y_test_pred, output_dict=True)\n",
        "\n",
        "# Per-class metrics (from report)\n",
        "beg_prec = report['Beginner']['precision']\n",
        "beg_rec  = report['Beginner']['recall']\n",
        "beg_f1   = report['Beginner']['f1-score']\n",
        "\n",
        "int_prec = report['Intermediate']['precision']\n",
        "int_rec  = report['Intermediate']['recall']\n",
        "int_f1   = report['Intermediate']['f1-score']\n",
        "\n",
        "adv_prec = report['Advanced']['precision']\n",
        "adv_rec  = report['Advanced']['recall']\n",
        "adv_f1   = report['Advanced']['f1-score']\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"BEST MODEL: Gradient Boosting\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "print(\"\\nWhy Gradient Boosting over alternatives:\")\n",
        "print(f\"[OK] Test accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"[OK] Test F1-macro: {test_f1_macro:.3f}\")\n",
        "print(f\"[OK] Beginner recall: {beg_rec:.1%}\")\n",
        "print(f\"[OK] Advanced precision: {adv_prec:.1%}\")\n",
        "print(f\"[OK] Train-test gap: {train_test_gap:.3f} (< 0.10 threshold)\")\n",
        "print(\"[OK] Handles non-linear interactions (skip_rate × total_sets)\")\n",
        "\n",
        "def row_of(name: str):\n",
        "    return df_results.loc[df_results['model'].eq(name)].iloc[0]\n",
        "\n",
        "lr = row_of('Logistic Regression')\n",
        "xg = row_of('XGBoost')\n",
        "\n",
        "print(\"\\nWhy NOT Logistic Regression:\")\n",
        "print(f\"[X] Lower test accuracy: {lr['test_acc']:.1%} vs {test_accuracy:.1%}\")\n",
        "print(f\"[OK] Better gap: {lr['gap']:.3f} vs {train_test_gap:.3f}\")\n",
        "\n",
        "print(\"\\nWhy NOT XGBoost:\")\n",
        "print(f\"[X] Larger gap: {xg['gap']:.3f} (threshold 0.10)\")\n",
        "print(f\"[X] Lower test accuracy: {xg['test_acc']:.1%} vs {test_accuracy:.1%}\")\n",
        "\n",
        "# --- Save best model bundle (PKL) ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SAVING BEST MODEL\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "best_model_bundle = {\n",
        "    'model': gb_best,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': feature_names,\n",
        "    'target_encoding': target_encoding,\n",
        "    'model_name': 'Gradient Boosting',\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_f1_macro': float(test_f1_macro),\n",
        "    'test_precision_macro': float(test_precision_macro),\n",
        "    'test_recall_macro': float(test_recall_macro),\n",
        "    'train_test_gap': float(train_test_gap),\n",
        "    'hyperparameters': gb_best.get_params(),\n",
        "    'feature_importance': feature_df.to_dict('records'),\n",
        "    'version': '2.2',\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "model_path = MODELDIR / 'status_best_model_v2.2.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(best_model_bundle, f)\n",
        "\n",
        "print(f\"\\n[OK] Best model saved: {model_path}\")\n",
        "print(f\"  File size: {model_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# --- Save model info (JSON) ---\n",
        "model_info = {\n",
        "    'model_name': 'Gradient Boosting',\n",
        "    'version': '2.2',\n",
        "    'date_trained': datetime.now().isoformat(),\n",
        "    'dataset': {\n",
        "        'n_train': int(len(X_train)),\n",
        "        'n_test': int(len(X_test)),\n",
        "        'n_features': int(len(feature_names)),\n",
        "        'features': feature_names\n",
        "    },\n",
        "    'performance': {\n",
        "        'test_accuracy': float(test_accuracy),\n",
        "        'test_f1_macro': float(test_f1_macro),\n",
        "        'test_precision_macro': float(test_precision_macro),\n",
        "        'test_recall_macro': float(test_recall_macro),\n",
        "        'train_test_gap': float(train_test_gap),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'hyperparameters': gb_best.get_params(),\n",
        "    'feature_importance_top3': feature_df.head(3).to_dict('records')\n",
        "}\n",
        "\n",
        "info_path = MODELDIR / 'status_model_info_v2.2.json'\n",
        "with open(info_path, 'w') as f:\n",
        "    json.dump(model_info, f, indent=2, default=float)  # robust vs numpy types [web:10][web:575]\n",
        "\n",
        "print(f\"[OK] Model info saved: {info_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 8 - SUMMARY**"
      ],
      "metadata": {
        "id": "3JJINWFg8gQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 7: SUMMARY\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATUS MODELING v2.2 - COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n┌\" + \"─\"*78 + \"┐\")\n",
        "print(\"│\" + \" \"*26 + \"MODELING COMPLETE\" + \" \"*35 + \"│\")\n",
        "print(\"└\" + \"─\"*78 + \"┘\")\n",
        "\n",
        "print(\"\\nBEST MODEL\")\n",
        "print(\"-\"*80)\n",
        "print(\"Model:           Gradient Boosting\")\n",
        "print(f\"Test Accuracy:   {test_accuracy:.1%}\")\n",
        "print(f\"F1-macro:        {test_f1_macro:.3f}\")\n",
        "print(f\"Train-Test Gap:  {train_test_gap:.3f} (< 0.10 [OK])\")\n",
        "\n",
        "print(\"\\nPER-CLASS PERFORMANCE\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Beginner:        Precision {beg_prec:.1%}, Recall {beg_rec:.1%}, F1 {beg_f1:.1%}\")\n",
        "print(f\"Intermediate:    Precision {int_prec:.1%}, Recall {int_rec:.1%}, F1 {int_f1:.1%}\")\n",
        "print(f\"Advanced:        Precision {adv_prec:.1%}, Recall {adv_rec:.1%}, F1 {adv_f1:.1%}\")\n",
        "\n",
        "print(\"\\nOUTPUT FILES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"[OK] {model_path}\")\n",
        "print(f\"[OK] {info_path}\")\n",
        "print(f\"[OK] {comparison_path}\")\n",
        "print(f\"[OK] {cm_path}\")\n",
        "print(f\"[OK] {fi_path}\")\n",
        "print(f\"[OK] {comp_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"STATUS MODULE COMPLETE ({test_accuracy:.1%} accuracy achieved!)\")\n",
        "print(\"=\"*80)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTA59sdN8il0",
        "outputId": "20b0a643-0c1a-4fae-fd9a-16008a9fccd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STATUS MODELING v2.2 - COMPLETE\n",
            "================================================================================\n",
            "\n",
            "┌──────────────────────────────────────────────────────────────────────────────┐\n",
            "│                          MODELING COMPLETE                                   │\n",
            "└──────────────────────────────────────────────────────────────────────────────┘\n",
            "\n",
            "BEST MODEL\n",
            "--------------------------------------------------------------------------------\n",
            "Model:           Gradient Boosting\n",
            "Test Accuracy:   95.1%\n",
            "F1-macro:        0.951\n",
            "Train-Test Gap:  0.049 (< 0.10 [OK])\n",
            "\n",
            "PER-CLASS PERFORMANCE\n",
            "--------------------------------------------------------------------------------\n",
            "Beginner:        Precision 97.1%, Recall 100.0%, F1 98.6%\n",
            "Intermediate:    Precision 91.4%, Recall 94.1%, F1 92.8%\n",
            "Advanced:        Precision 96.9%, Recall 91.2%, F1 93.9%\n",
            "\n",
            "OUTPUT FILES\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] models/status_best_model_v2.2.pkl\n",
            "[OK] models/status_model_info_v2.2.json\n",
            "[OK] models/status_model_comparison_v2.2.json\n",
            "[OK] visualizations/STATUS_Modeling_v2.2/confusion_matrix_best_model_v2.2.png\n",
            "[OK] visualizations/STATUS_Modeling_v2.2/feature_importance_best_model_v2.2.png\n",
            "[OK] visualizations/STATUS_Modeling_v2.2/model_comparison_v2.2.png\n",
            "\n",
            "================================================================================\n",
            "STATUS MODULE COMPLETE (95.1% accuracy achieved!)\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}